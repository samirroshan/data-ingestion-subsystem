# Data Ingestion Subsystem

The **Data Ingestion Subsystem** is a production-style ETL component built in Python that processes **IMDB Movie Rating Data** from CSV files. The dataset includes detailed movie information and rating metrics such as:

- Title  
- Genre  
- Description  
- Director  
- Cast  
- Release year  
- Runtime (minutes)  
- **IMDB rating (user score)**  
- **Votes (movie popularity)**  
- **Revenue (in millions USD)**  
- **Metascore (critic rating)**  

This subsystem performs:

- **Extraction** of raw IMDB movie rating data  
- **Validation & transformation** using a configurable schema  
- **Loading** into PostgreSQL staging and reject tables  
- **Error tracking** for invalid or inconsistent movie records  

It is designed to mimic how real data engineering teams handle ingestion and data quality pipelines.

---

## Features

- ğŸ§¾ **YAML-driven configuration** for flexible dataset definitions  
- âœ… **Row-level validation** with typed fields and error messages  
- ğŸ§± **Staging table** for clean, validated movie records  
- âŒ **Reject table** for invalid rows (with error reasons)  
- ğŸ§ª **PyTest suite** for validation and reliability  
- ğŸ” Can ingest additional datasets by simply creating new YAML configs  

---

## Dataset Description â€” IMDB Movie Rating Data

The system ingests **IMDB movie rating data**, a structured dataset containing:

- Rank  
- Movie title  
- Genre  
- Description  
- Director  
- Actors  
- Year of release  
- Runtime (minutes)  
- **IMDB user rating (float)**  
- **Number of votes submitted by users**  
- **Box office revenue in millions USD**  
- **Metascore (critic-based rating)**  

These fields are validated and cleaned using the schema defined in the YAML configuration file.

---

## How to RUN?
# ğŸš€ How to Run the IMDB Data Ingestion Pipeline

This project provides a complete ETL-style data ingestion subsystem for validating and loading IMDB movie data into a PostgreSQL database.

---

## 1. Clone the Repository

```bash
git clone https://github.com/samirroshan/data-ingestion-subsystem.git
cd data-ingestion-subsystem
```

---

## 2. Create & Activate the Virtual Environment

```bash
python -m venv venv
source venv/bin/activate      # macOS / Linux
# venv\Scripts\activate       # Windows
```

---

## 3. Install Dependencies

```bash
pip install -r requirements.txt
```

If no `requirements.txt` exists:

```bash
pip install pandas psycopg2-binary pyyaml pytest pytest-cov
```

---

## 4. Set Up PostgreSQL Tables

```bash
createdb imdb
```

Then open `psql` and run:

```sql
CREATE TABLE IF NOT EXISTS stg_movies (
    rank_num          INTEGER,
    title             TEXT,
    genre             TEXT,
    description       TEXT,
    director          TEXT,
    actors            TEXT,
    year              INTEGER,
    runtime_minutes   INTEGER,
    rating            DOUBLE PRECISION,
    votes             INTEGER,
    revenue_millions  DOUBLE PRECISION,
    metascore         DOUBLE PRECISION
);

CREATE TABLE IF NOT EXISTS stg_rejects (
    id           SERIAL PRIMARY KEY,
    source_file  TEXT,
    raw_record   JSONB,
    error_reason TEXT,
    created_at   TIMESTAMP DEFAULT NOW()
);
```

---

## 5. Configure Your `config.yaml`

```yaml
database:
  name: imdb
  user: postgres
  password: your_password_here
  host: localhost
  port: 5432

paths:
  source_csv: data/imdb_movie_dataset.csv
  rejected_csv: outputs/rejected_rows.csv
  log_file: outputs/ingestion.log
```

Make sure the CSV exists at:

```
data/imdb_movie_dataset.csv
```

---

## 6. Run the Pipeline

From the project root:

```bash
# Run Main Ingestion (Python)
python -m src.Main.ingestion_flow

# Run Spark Ingestion
python -m src.load.load_imdb

# Run Rejects Loader
python -m src.load.load_rejects_to_db
```

### Expected Output

```
Read 1000 rows from data/imdb_movie_dataset.csv
Inserted 838 rows into stg_movies
Rejected 162 rows into stg_rejects
Wrote 10056 cleaned rows to outputs/clean_imdb_movies.csv
```

### What Happens in the Run

- Extract raw CSV rows  
- Validate each row using custom business rules  
- Insert valid rows â†’ `stg_movies`  
- Insert invalid rows â†’ `stg_rejects` + CSV reject log  
- Generate analytics-ready clean dataset â†’ `outputs/clean_imdb_movies.csv`

---

## 7. Run Unit Tests & Coverage

```bash
pytest --cov=src
```

---

## âœ… Pipeline Ready

You now have a fully functional, reproducible ETL ingestion subsystem.


## Project Structure

```text
data-ingestion-subsystem/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ imdb_movie_dataset.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Main/
â”‚   â”‚   â”œâ”€â”€ ingestion_flow.py
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ load_imdb.py
â”‚   â”‚   â”œâ”€â”€ load_rejects_to_db.py
â”‚   â”‚   â”œâ”€â”€ loaders.py
â”‚   â”‚   â””â”€â”€ db.py
â”‚   â”œâ”€â”€ reader/
â”‚   â”‚   â””â”€â”€ data_reader.py
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ transformers.py
â”‚   â””â”€â”€ validator/
â”‚       â””â”€â”€ validator.py
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
